This project focuses on character-level adversarial attacks in the context of text classification using machine learning. It was developed as part of a homework assignment for the Ethical AI course, with the objective of exploring the vulnerabilities of standard machine learning models to adversarial perturbationsâ€”specifically in natural language processing (NLP) tasks.

The assignment revolves around designing both untargeted and targeted adversarial attacks on a trained sentiment classification model, using only character-level modifications. The project demonstrates how even slight, human-imperceptible changes to text can dramatically affect model predictionsâ€”raising concerns around the robustness and ethical deployment of AI systems in real-world applications.

The entire task is implemented in Python using the scikit-learn library, with the model trained on the Rotten Tomatoes dataset using a Logistic Regression classifier and TF-IDF vectorizer.

â¸»

ðŸ§  Problem Description

The primary goal was to evaluate the susceptibility of a sentiment classification model to adversarial examplesâ€”texts intentionally perturbed to fool the model. The problem was divided into four structured parts:

â¸»

ðŸ”¹ Part 1 â€“ Model Training and Evaluation
	â€¢	Trained a Logistic Regression classifier using TF-IDF vectorization on the Rotten Tomatoes dataset.
	â€¢	Evaluated the modelâ€™s performance on both training and testing sets using standard metrics: accuracy, precision, recall, F1 score, and confusion matrix.
	â€¢	Observed the modelâ€™s generalization capabilities and baseline behavior before introducing any adversarial modifications.

â¸»

ðŸ”¹ Part 2 â€“ Character-Level Modification Function
	â€¢	Created a character_modify() function capable of applying various random character-level edits to individual words.
	â€¢	Implemented multiple perturbation strategies:
	â€¢	Whitespace insertion
	â€¢	Character substitution (e.g., â€˜aâ€™ â†’ â€˜@â€™)
	â€¢	Character swapping
	â€¢	Character deletion
	â€¢	Character addition
	â€¢	The function was designed with built-in randomness and modularity to support both untargeted and targeted attack strategies.

â¸»

ðŸ”¹ Part 3 â€“ Untargeted Attack
	â€¢	Developed an untargeted_attack() function that randomly perturbs 40% of the words in a given text using the character_modify() logic.
	â€¢	Applied the attack on test data and saved the adversarial examples.
	â€¢	Evaluated the modelâ€™s performance on these modified examples across five separate runs, calculating the average drop in classification performance.
	â€¢	Analyzed how random modifications impacted readability and the classifierâ€™s robustness.

â¸»

ðŸ”¹ Part 4 â€“ Targeted Attack
	â€¢	Designed a targeted_attack() function using a greedy selection algorithm to identify the most impactful words in a textâ€”those that, when removed, cause the greatest drop in model confidence.
	â€¢	Strategically modified only the most critical words using character_modify() until the model changed its prediction.
	â€¢	Performed the attack only on test texts that were initially predicted correctly, to ensure effectiveness.
	â€¢	Evaluated the modelâ€™s performance over five separate runs and compared it with the untargeted attack.
	â€¢	Observed significantly higher performance degradation with targeted attacks, confirming the modelâ€™s reliance on specific influential words.

â¸»

ðŸ“Š Key Takeaways
	â€¢	Untargeted attacks caused a moderate performance drop but maintained sentence readability.
	â€¢	Targeted attacks led to a drastic drop in model accuracy and interpretability, often fooling the model by modifying a handful of critical words.
	â€¢	The experiments demonstrate the importance of robustness in NLP models and the ethical implications of deploying models in sensitive contexts where adversarial inputs may be used maliciously.
