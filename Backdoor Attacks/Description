üìå Backdoor Attack on Misinformation Detection using Logistic Regression

This project explores how backdoor attacks can manipulate machine learning classifiers in the domain of misinformation detection. The approach involves poisoning the training dataset by injecting specific trigger words or phrases into non-misinformation examples. When the trained model later sees the same trigger in a misinformation input, it is tricked into misclassifying it as non-misinformation.

üîç Key Features
	‚Ä¢	Data Poisoning Functionality: Modular function to inject triggers into non-misinformation samples at configurable poison rates.
	‚Ä¢	Model Training & Evaluation: Logistic Regression model trained on poisoned datasets and evaluated against both clean and poisoned test sets.
	‚Ä¢	Trigger Variants:
	‚Ä¢	A non-existent word (mamba2408)
	‚Ä¢	An existing word (Kobe)
	‚Ä¢	A phrase (greatest scorer of all time)
	‚Ä¢	Poison Rate Experiments: Tested across 5 poison rates ‚Äî 0.01, 0.1, 0.25, 0.5, and 0.9 ‚Äî for all trigger types.
	‚Ä¢	Visualization: Line graph comparing poisoned test accuracy across different triggers and poison rates.
	‚Ä¢	Confusion Matrix Analysis: Detailed inspection of how backdoor attacks increase false negatives and reduce true positives on poisoned inputs.

üìà Observations
	‚Ä¢	Phrase-based triggers were more effective in degrading model performance, especially at higher poison rates.
	‚Ä¢	Non-existent and real-word triggers had similar impacts, indicating the model‚Äôs generalization over unseen tokens.
	‚Ä¢	Even low poison rates (e.g., 1%) introduced backdoors with measurable effect, suggesting potential stealthy attack avenues.

üìÇ Project Structure
	‚Ä¢	USERNAME_EC2.py: Core script containing poisoning and training functions.
	‚Ä¢	report.docx: Full write-up including methodology, visualizations, tables, and analysis.
