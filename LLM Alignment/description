This project explores a simple approach to aligning language model outputs with human preferences using a lightweight RLHF-inspired framework. The workflow involves generating paired responses (agree/disagree) from an open-source language model, labeling them based on human preference, and training a logistic regression classifier to score new generations.

ðŸ”§ Whatâ€™s Included:
	â€¢	Generation of preference-based responses using google/flan-t5-base
	â€¢	Manual labeling of generated outputs to reflect preferred responses
	â€¢	Training a scoring function using Logistic Regression and TF-IDF embeddings
	â€¢	Re-ranking LLM outputs based on the trained scoring model
	â€¢	Output files containing original prompts, generated texts, and model scores

ðŸ¤– Technologies Used:
	â€¢	Hugging Face Transformers (FLAN-T5)
	â€¢	scikit-learn (TF-IDF, Logistic Regression)
	â€¢	Python (pandas, csv, basic file handling)

ðŸ’¡ Key Concepts:
	â€¢	Prompt-based generation
	â€¢	Text classification using logistic regression
	â€¢	Scoring and reranking LLM responses
	â€¢	Human preference modeling with minimal supervision
