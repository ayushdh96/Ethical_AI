This repository contains an extension of adversarial attack for the untargeted attacks aimed at improving the robustness of a Logistic Regression (LR) sentiment classifier trained on the Rotten Tomatoes movie review dataset. The focus of this work is on adversarial training â€” a technique where we enhance the modelâ€™s generalization and resilience by exposing it to intelligently perturbed inputs (adversarial examples) during training.

â¸»

ğŸ¯ Project Objective

The primary goal is to fortify the Logistic Regression model from HW4 against untargeted character-level adversarial attacks. This is accomplished by generating k adversarial variants per training example using a custom attack function and retraining the model on this enriched dataset.

â¸»

ğŸ› ï¸ Core Components
	1.	adversarial_training()
	â€¢	For each example in the training set, generates k adversarial examples using the untargeted attack function (from HW4).
	â€¢	Saves both original and adversarial examples (along with their labels) into a single CSV file: adversarial_train_rotten-tomatoes.csv.
	â€¢	Ensures the test set is untouched, preserving proper evaluation boundaries.
	2.	Training a New Model
	â€¢	Trains a new Logistic Regression model on the augmented training set with a fresh TF-IDF vectorizer.
	â€¢	Mimics the training pipeline from HW4 but leverages a more diverse dataset.
	3.	Evaluation Function
	â€¢	Tests both the original and adversarial-trained models on:
	â€¢	Original test set
	â€¢	Adversarially perturbed test set
	â€¢	Reports evaluation metrics: Accuracy, Precision, Recall, and F1 Score.
	4.	Experimental Loop
	â€¢	Runs the evaluation for multiple values of k âˆˆ [1, 2, 3, 4, 5].
	â€¢	Each configuration is repeated 5 times to account for randomness in adversarial example generation.
	â€¢	Averages the results across runs to report stable performance measures.

â¸»

ğŸ“Š Final Report Includes
	â€¢	A comprehensive table summarizing the performance of:
	â€¢	Original model on clean and adversarial test data.
	â€¢	Adversarially trained model across all k values on both test types.
	â€¢	A detailed discussion and analysis, including:
	â€¢	Which model variant performs best and why.
	â€¢	Whether adversarial training successfully mitigates the attack.
	â€¢	Limitations of the approach and suggestions for further robustness improvements.
	â€¢	Optionally: Graphs or visuals to support findings.

â¸»

ğŸ“š Dependencies
	â€¢	Python 3.x
	â€¢	pandas, numpy
	â€¢	sklearn (for Logistic Regression, TF-IDF, and metrics)

â¸»

ğŸ“Œ Note

This project is self-contained and does not rely on external test data leaks. It includes its own .py file and a separate written report.
